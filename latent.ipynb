{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from glob import glob\n",
    "import os\n",
    "from monai.networks.nets import SwinUNETR\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT CORRESPONDING LATENTS\n",
    "\n",
    "def get_random_crop_coords(crop_size=[128, 128, 128]):\n",
    "    # Input shape is fixed as 240x240x155\n",
    "    input_shape = [240, 240, 155]\n",
    "\n",
    "    # Calculate the possible starting indices for the crop\n",
    "    max_x = input_shape[0] - crop_size[0]  # 240 - 128\n",
    "    max_y = input_shape[1] - crop_size[1]  # 240 - 128\n",
    "    max_z = input_shape[2] - crop_size[2]  # 155 - 128\n",
    "\n",
    "    # Randomly select the starting indices for the crop\n",
    "    start_x = random.randint(0, max_x)\n",
    "    start_y = random.randint(0, max_y)\n",
    "    start_z = random.randint(0, max_z)\n",
    "\n",
    "    # Return the starting coordinates\n",
    "    return start_x, start_y, start_z\n",
    "\n",
    "class LatentSwinUNETR(SwinUNETR):\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        if not torch.jit.is_scripting():\n",
    "            self._check_input_size(x_in.shape[2:])\n",
    "        hidden_states_out = self.swinViT(x_in, self.normalize)\n",
    "        dec4 = self.encoder10(hidden_states_out[4])\n",
    "        return dec4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_path = \"/mnt/disk1/hjlee/orhun/data/BRATS_Preprocessed/Images\"\n",
    "\n",
    "# Get a list of all .nii.gz files in the directory and its subdirectories\n",
    "nii_files = glob(os.path.join(search_path, '**', '*.nii.gz'), recursive=True)\n",
    "\n",
    "coords = {}\n",
    "for file in nii_files:\n",
    "    coords[file]= get_random_crop_coords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/hjlee/orhun/envs/multimodalenv/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING MODEL:  /mnt/disk1/hjlee/orhun/repo/thesis/models/BRATS_FLAIR_SwinUnetr/BRATS_FLAIR_SwinUnetr_checkpoint_Epoch_349.pt\n"
     ]
    }
   ],
   "source": [
    "DEVICE_ID = 3\n",
    "cuda_id = \"cuda:\" + str(DEVICE_ID)\n",
    "device = torch.device(cuda_id)\n",
    "torch.cuda.set_device(DEVICE_ID)    \n",
    "\n",
    "channels = [\"FLAIR\", \"T1\", \"T1c\", \"T2\"]\n",
    "\n",
    "model = LatentSwinUNETR(\n",
    "    img_size=(128, 128, 128),\n",
    "    in_channels= 1,  #len(channels)\n",
    "    out_channels=1,\n",
    "    feature_size=48,\n",
    "    use_checkpoint=True).to(device)\n",
    "\n",
    "load_model_path = \"/mnt/disk1/hjlee/orhun/repo/thesis/models/BRATS_FLAIR_SwinUnetr/BRATS_FLAIR_SwinUnetr_checkpoint_Epoch_349.pt\"\n",
    "\n",
    "print(\"LOADING MODEL: \", load_model_path)\n",
    "checkpoint = torch.load(load_model_path, map_location={\"cuda:0\":cuda_id,\"cuda:1\":cuda_id})\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "search_path = \"/mnt/disk1/hjlee/orhun/data/BRATS_Preprocessed/Images\"\n",
    "\n",
    "# Get a list of all .nii.gz files in the directory and its subdirectories\n",
    "nii_files = glob(os.path.join(search_path, '**', '*.nii.gz'), recursive=True)\n",
    "\n",
    "output_path = \"/mnt/disk1/hjlee/orhun/data/BRATS_Latents/FLAIR\"\n",
    "\n",
    "for file in nii_files:\n",
    "    img = nib.load(file)\n",
    "    img_array=np.array(img.dataobj)\n",
    "    local_coords = coords[file]\n",
    "    cropped_img = img_array[local_coords[0]:local_coords[0]+ 128, \n",
    "                              local_coords[1]:local_coords[1] + 128, \n",
    "                              local_coords[2]:local_coords[2] + 128, :]\n",
    "    cropped_img = np.transpose(cropped_img, (3, 0, 1, 2))\n",
    "    cropped_tensor = torch.from_numpy(cropped_img).unsqueeze(0)[:, 0:1, ...].to(device)\n",
    "    #cropped_tensor = torch.from_numpy(cropped_img).unsqueeze(0).to(device)\n",
    "    \n",
    "    out = model(cropped_tensor)\n",
    "\n",
    "    base_filename = os.path.basename(file).replace('.nii.gz', '.pt')\n",
    "    tensor_save_path = os.path.join(output_path, base_filename)\n",
    "    torch.save(out, tensor_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = nib.load(\"/mnt/disk1/hjlee/orhun/data/BRATS_Preprocessed/Images/BRATS_001.nii.gz\")\n",
    "label = nib.load(\"/mnt/disk1/hjlee/orhun/data/BRATS_Preprocessed/Labels/BRATS_001.nii.gz\")\n",
    "affine=img.affine\n",
    "img_array=np.array(img.dataobj)\n",
    "label_array=np.array(label.dataobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT RANDOM CROPS\n",
    "\n",
    "class LatentSwinUNETR(SwinUNETR):\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        if not torch.jit.is_scripting():\n",
    "            self._check_input_size(x_in.shape[2:])\n",
    "        hidden_states_out = self.swinViT(x_in, self.normalize)\n",
    "        dec4 = self.encoder10(hidden_states_out[4])\n",
    "        return dec4\n",
    "\n",
    "def cropRandom(input_tensor, crop_size):\n",
    "    # Calculate the possible starting indices for the crop\n",
    "    max_x = input_tensor.shape[0] - crop_size[0]\n",
    "    max_y = input_tensor.shape[1] - crop_size[1]\n",
    "    max_z = input_tensor.shape[2] - crop_size[2]\n",
    "\n",
    "    # Randomly select the starting indices for the crop\n",
    "    start_x = random.randint(0, max_x)\n",
    "    start_y = random.randint(0, max_y)\n",
    "    start_z = random.randint(0, max_z)\n",
    "\n",
    "    # Crop the cube from the input tensor\n",
    "    cropped_tensor = input_tensor[start_x:start_x + crop_size[0], \n",
    "                                start_y:start_y + crop_size[1], \n",
    "                                start_z:start_z + crop_size[2], :]\n",
    "\n",
    "    return cropped_tensor\n",
    "\n",
    "DEVICE_ID = 7\n",
    "cuda_id = \"cuda:\" + str(DEVICE_ID)\n",
    "device = torch.device(cuda_id)\n",
    "torch.cuda.set_device(DEVICE_ID)    \n",
    "\n",
    "cropped_input_size = [128,128,128]\n",
    "channels = [\"FLAIR\", \"T1\", \"T1c\", \"T2\"]\n",
    "\n",
    "model = LatentSwinUNETR(\n",
    "    img_size=(cropped_input_size[0], cropped_input_size[1], cropped_input_size[2]),\n",
    "    in_channels=1,#in_channels=len(channels),\n",
    "    out_channels=1,\n",
    "    feature_size=48,\n",
    "    use_checkpoint=True).to(device)\n",
    "\n",
    "\n",
    "load_model_path = \"/mnt/disk1/hjlee/orhun/repo/MultiUnet/models/BRATSFLAIR_latent_checkpoint_Epoch_599.pt\"\n",
    "print(\"LOADING MODEL: \", load_model_path)\n",
    "checkpoint = torch.load(load_model_path, map_location={\"cuda:0\":cuda_id,\"cuda:1\":cuda_id})\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "search_path = \"/mnt/disk1/hjlee/orhun/data/BRATS_Preprocessed/Images\"\n",
    "\n",
    "# Get a list of all .nii.gz files in the directory and its subdirectories\n",
    "nii_files = glob(os.path.join(search_path, '**', '*.nii.gz'), recursive=True)\n",
    "\n",
    "output_path = \"/mnt/disk1/hjlee/orhun/data/BRATS_Preprocessed/LatentsFLAIR\"\n",
    "\n",
    "for file in nii_files:\n",
    "    img = nib.load(file)\n",
    "    img_array=np.array(img.dataobj)\n",
    "    cropped_img = cropRandom(img_array,cropped_input_size)\n",
    "    cropped_img = np.transpose(cropped_img, (3, 0, 1, 2))\n",
    "    cropped_tensor = torch.from_numpy(cropped_img).unsqueeze(0)[:, 0:1, ...].to(device)\n",
    "\n",
    "    out = model(cropped_tensor)\n",
    "\n",
    "    base_filename = os.path.basename(file).replace('.nii.gz', '.pt')\n",
    "    tensor_save_path = os.path.join(output_path, base_filename)\n",
    "    torch.save(out, tensor_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_latent = torch.load('/mnt/disk1/hjlee/orhun/data/BRATS_Preprocessed/LatentsFLAIR/BRATS_001.pt', weights_only=True)\n",
    "all_latent = torch.load('/mnt/disk1/hjlee/orhun/data/BRATS_Preprocessed/Latents/BRATS_001.pt', weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 4, 4, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 4, 4, 4])\n",
      "torch.Size([1, 256, 1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1, 256])\n",
      "torch.Size([1, 1, 1, 1, 256])\n",
      "torch.Size([256, 1, 1])\n",
      "torch.Size([256, 1, 1])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "was expecting embedding dimension of 256, but got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     recon, vq_loss \u001b[38;5;241m=\u001b[39m \u001b[43mlatent_diffusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflair_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Compute reconstruction loss\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     recon_loss \u001b[38;5;241m=\u001b[39m criterion(recon, multimodal_embeddings)\n",
      "File \u001b[0;32m/mnt/disk1/hjlee/orhun/envs/multimodalenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/disk1/hjlee/orhun/repo/MultiUnet/nets/vqvae.py:114\u001b[0m, in \u001b[0;36mLatentDiffusionModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m _, vq_loss, z_quantized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvqvae(x)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Apply transformer-based latent diffusion\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m z_diffused \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatent_diffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_quantized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Decode back to multi-modal space\u001b[39;00m\n\u001b[1;32m    117\u001b[0m x_recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvqvae\u001b[38;5;241m.\u001b[39mdecoder(z_diffused)\n",
      "File \u001b[0;32m/mnt/disk1/hjlee/orhun/envs/multimodalenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/disk1/hjlee/orhun/repo/MultiUnet/nets/vqvae.py:99\u001b[0m, in \u001b[0;36mLatentDiffusionTransformer.forward\u001b[0;34m(self, z_quantized)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(z\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks:\n\u001b[0;32m---> 99\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m z_latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_latent(z)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z_latent\u001b[38;5;241m.\u001b[39mview(z_quantized\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/mnt/disk1/hjlee/orhun/envs/multimodalenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/disk1/hjlee/orhun/repo/MultiUnet/nets/vqvae.py:76\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Self-attention layer\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 76\u001b[0m     attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m attn_output)\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Feed-forward network\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/disk1/hjlee/orhun/envs/multimodalenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/disk1/hjlee/orhun/envs/multimodalenv/lib/python3.10/site-packages/torch/nn/modules/activation.py:1167\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1157\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1158\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         q_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj_weight, k_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1165\u001b[0m         v_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj_weight, average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/mnt/disk1/hjlee/orhun/envs/multimodalenv/lib/python3.10/site-packages/torch/nn/functional.py:5026\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5023\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _kpm_dtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbool \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_floating_point(key_padding_mask):\n\u001b[1;32m   5024\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m   5025\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly bool and floating types of key_padding_mask are supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 5026\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m embed_dim \u001b[38;5;241m==\u001b[39m embed_dim_to_check, \\\n\u001b[1;32m   5027\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas expecting embedding dimension of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membed_dim_to_check\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membed_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embed_dim, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m   5029\u001b[0m     \u001b[38;5;66;03m# embed_dim can be a tensor when JIT tracing\u001b[39;00m\n\u001b[1;32m   5030\u001b[0m     head_dim \u001b[38;5;241m=\u001b[39m embed_dim\u001b[38;5;241m.\u001b[39mdiv(num_heads, rounding_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrunc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: was expecting embedding dimension of 256, but got 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from nets.vqvae import LatentDiffusionModel\n",
    "\n",
    "torch.cuda.set_device(7)   \n",
    "\n",
    "\n",
    "flair_embeddings = torch.load('/mnt/disk1/hjlee/orhun/data/BRATS_Preprocessed/LatentsFLAIR/BRATS_001.pt').cuda()\n",
    "multimodal_embeddings = torch.load('/mnt/disk1/hjlee/orhun/data/BRATS_Preprocessed/Latents/BRATS_001.pt').cuda()\n",
    "\n",
    "# Initialize the latent diffusion model\n",
    "latent_diffusion_model = LatentDiffusionModel(\n",
    "    in_channels=768, embedding_dim=256, num_embeddings=512, \n",
    "    commitment_cost=0.25, num_layers=4, num_heads=8, ff_dim=1024\n",
    ").cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(latent_diffusion_model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    recon, vq_loss = latent_diffusion_model(flair_embeddings)\n",
    "    \n",
    "    # Compute reconstruction loss\n",
    "    recon_loss = criterion(recon, multimodal_embeddings)\n",
    "    total_loss = recon_loss + vq_loss\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple LDM\n",
    "\n",
    "from nets.vqvae import LatentUNet, NoiseScheduler, LatentDiffusionModel\n",
    "from latent_dataset import LatentEmbeddingsDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "torch.cuda.set_device(6)  \n",
    "_date = datetime.now().strftime(\"%d-%H-%M\")\n",
    "#log_dir = f\"./runs/diff_latent_{_date}\"\n",
    "log_dir = \"./runs/test\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Define paths to your latent embedding directories\n",
    "latents_dir = '/mnt/disk1/hjlee/orhun/data/BRATS_Preprocessed/CorrespondingLatents/All'  # Replace with the correct path\n",
    "latents_single_dir = '/mnt/disk1/hjlee/orhun/data/BRATS_Preprocessed/CorrespondingLatents/T1C'  # Replace with the correct path\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "num_epochs = 600\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = LatentEmbeddingsDataset(latents_dir, latents_single_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the Latent Diffusion Model (with latent dimension and conditioning dimension both set to 768)\n",
    "latent_dim = 768\n",
    "condition_dim = 768\n",
    "model = LatentDiffusionModel(latent_dim=latent_dim, condition_dim=condition_dim).cuda()\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# Train the model\n",
    "\n",
    "def train_diffusion_model(model, dataloader, num_epochs):\n",
    "    noise_scheduler = NoiseScheduler()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        step = 0\n",
    "        for source_embeddings, target_embeddings in dataloader:\n",
    "            source_embeddings = source_embeddings.cuda()  # Move to GPU\n",
    "            target_embeddings = target_embeddings.cuda()\n",
    "            step+=1\n",
    "            # Randomly select timesteps for each batch\n",
    "            t = torch.randint(0, 1000, (target_embeddings.size(0),)).cuda()\n",
    "\n",
    "            # Add noise to the target embeddings\n",
    "            noisy_embeddings = noise_scheduler.add_noise(target_embeddings, t)\n",
    "\n",
    "            # Forward pass through the diffusion model\n",
    "            optimizer.zero_grad()\n",
    "            output = model(source_embeddings, noisy_embeddings, t)\n",
    "\n",
    "            # Compute loss (MSE between the predicted and clean embeddings)\n",
    "            loss = F.mse_loss(output, target_embeddings)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(dataloader)}')\n",
    "        writer.add_scalar(\"DiffTraining/EpochLoss\", epoch_loss / len(dataloader), epoch)\n",
    "    \n",
    "    torch.save(model.state_dict(), \"/mnt/disk1/hjlee/orhun/repo/MultiUnet/models/t1c_all_diff_latent_600_1e-5.pt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "Epoch 1/600, Loss: 0.49739476609027994\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "Epoch 2/600, Loss: 0.05527676597742711\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "Epoch 3/600, Loss: 0.01101581760982083\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "Epoch 4/600, Loss: 0.003314124828725422\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "Epoch 5/600, Loss: 0.0012855188382426435\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "Epoch 6/600, Loss: 0.0005854283172392555\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "Epoch 7/600, Loss: 0.000302015575609896\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "Epoch 8/600, Loss: 0.00019245229486841708\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "Epoch 9/600, Loss: 0.00014368317931519537\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "Epoch 10/600, Loss: 0.00011283794348424111\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "Epoch 11/600, Loss: 0.00010759118354683979\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "Epoch 12/600, Loss: 9.942382881891424e-05\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "Epoch 13/600, Loss: 0.00011154481121516487\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "torch.Size([5, 768, 4, 4, 4])\n",
      "Epoch 14/600, Loss: 0.00010502968827988637\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n",
      "torch.Size([8, 768, 4, 4, 4])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_diffusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 66\u001b[0m, in \u001b[0;36mtrain_diffusion_model\u001b[0;34m(model, dataloader, num_epochs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     64\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 66\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     69\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiffTraining/EpochLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader), epoch)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_diffusion_model(model, dataloader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600, Loss: 1.022239327430725\n",
      "Epoch 2/600, Loss: 1.0090160369873047\n",
      "Epoch 3/600, Loss: 1.0070483684539795\n",
      "Epoch 4/600, Loss: 0.9973657131195068\n",
      "Epoch 5/600, Loss: 0.99908047914505\n",
      "Epoch 6/600, Loss: 1.0029687881469727\n",
      "Epoch 7/600, Loss: 0.9978360533714294\n",
      "Epoch 8/600, Loss: 0.9988768696784973\n",
      "Epoch 9/600, Loss: 0.9981911182403564\n",
      "Epoch 10/600, Loss: 1.001476764678955\n",
      "Epoch 11/600, Loss: 1.0017756223678589\n",
      "Epoch 12/600, Loss: 0.9985731244087219\n",
      "Epoch 13/600, Loss: 1.0010594129562378\n",
      "Epoch 14/600, Loss: 0.9968657493591309\n",
      "Epoch 15/600, Loss: 1.0043799877166748\n",
      "Epoch 16/600, Loss: 0.9951318502426147\n",
      "Epoch 17/600, Loss: 0.9969205856323242\n",
      "Epoch 18/600, Loss: 1.006028175354004\n",
      "Epoch 19/600, Loss: 1.007902979850769\n",
      "Epoch 20/600, Loss: 0.9992837309837341\n",
      "Epoch 21/600, Loss: 1.0037096738815308\n",
      "Epoch 22/600, Loss: 0.9978023767471313\n",
      "Epoch 23/600, Loss: 0.9993953108787537\n",
      "Epoch 24/600, Loss: 0.9959292411804199\n",
      "Epoch 25/600, Loss: 0.9961526989936829\n",
      "Epoch 26/600, Loss: 1.0014530420303345\n",
      "Epoch 27/600, Loss: 0.9997454881668091\n",
      "Epoch 28/600, Loss: 0.9970113635063171\n",
      "Epoch 29/600, Loss: 0.9988260269165039\n",
      "Epoch 30/600, Loss: 1.001800298690796\n",
      "Epoch 31/600, Loss: 0.998414158821106\n",
      "Epoch 32/600, Loss: 1.0043931007385254\n",
      "Epoch 33/600, Loss: 1.0033363103866577\n",
      "Epoch 34/600, Loss: 0.997877836227417\n",
      "Epoch 35/600, Loss: 1.0038964748382568\n",
      "Epoch 36/600, Loss: 1.0038775205612183\n",
      "Epoch 37/600, Loss: 1.001887559890747\n",
      "Epoch 38/600, Loss: 1.000443935394287\n",
      "Epoch 39/600, Loss: 0.9990789890289307\n",
      "Epoch 40/600, Loss: 0.9983293414115906\n",
      "Epoch 41/600, Loss: 0.9997981190681458\n",
      "Epoch 42/600, Loss: 0.9960379600524902\n",
      "Epoch 43/600, Loss: 1.0012413263320923\n",
      "Epoch 44/600, Loss: 0.9996874928474426\n",
      "Epoch 45/600, Loss: 0.9991137981414795\n",
      "Epoch 46/600, Loss: 0.9959794878959656\n",
      "Epoch 47/600, Loss: 1.0021299123764038\n",
      "Epoch 48/600, Loss: 1.0012791156768799\n",
      "Epoch 49/600, Loss: 0.9990941286087036\n",
      "Epoch 50/600, Loss: 0.9960271716117859\n",
      "Epoch 51/600, Loss: 1.000044584274292\n",
      "Epoch 52/600, Loss: 0.9983885884284973\n",
      "Epoch 53/600, Loss: 0.9987020492553711\n",
      "Epoch 54/600, Loss: 0.9989330768585205\n",
      "Epoch 55/600, Loss: 1.0041192770004272\n",
      "Epoch 56/600, Loss: 0.996324360370636\n",
      "Epoch 57/600, Loss: 1.0013622045516968\n",
      "Epoch 58/600, Loss: 0.9952322244644165\n",
      "Epoch 59/600, Loss: 1.0039745569229126\n",
      "Epoch 60/600, Loss: 0.9990047812461853\n",
      "Epoch 61/600, Loss: 1.0010044574737549\n",
      "Epoch 62/600, Loss: 1.0022200345993042\n",
      "Epoch 63/600, Loss: 1.0004802942276\n",
      "Epoch 64/600, Loss: 1.002208948135376\n",
      "Epoch 65/600, Loss: 0.9959650039672852\n",
      "Epoch 66/600, Loss: 0.9990827441215515\n",
      "Epoch 67/600, Loss: 0.9999299645423889\n",
      "Epoch 68/600, Loss: 0.9948318600654602\n",
      "Epoch 69/600, Loss: 0.9960866570472717\n",
      "Epoch 70/600, Loss: 1.0015392303466797\n",
      "Epoch 71/600, Loss: 0.995090901851654\n",
      "Epoch 72/600, Loss: 1.001417875289917\n",
      "Epoch 73/600, Loss: 0.9943382740020752\n",
      "Epoch 74/600, Loss: 0.9997914433479309\n",
      "Epoch 75/600, Loss: 0.9984989762306213\n",
      "Epoch 76/600, Loss: 0.9983445405960083\n",
      "Epoch 77/600, Loss: 0.9951694011688232\n",
      "Epoch 78/600, Loss: 1.0000816583633423\n",
      "Epoch 79/600, Loss: 1.0021183490753174\n",
      "Epoch 80/600, Loss: 1.000876784324646\n",
      "Epoch 81/600, Loss: 1.0007381439208984\n",
      "Epoch 82/600, Loss: 0.9988456964492798\n",
      "Epoch 83/600, Loss: 0.9999626874923706\n",
      "Epoch 84/600, Loss: 1.0025012493133545\n",
      "Epoch 85/600, Loss: 1.0046148300170898\n",
      "Epoch 86/600, Loss: 1.0002689361572266\n",
      "Epoch 87/600, Loss: 1.0021706819534302\n",
      "Epoch 88/600, Loss: 1.003820538520813\n",
      "Epoch 89/600, Loss: 0.9974542856216431\n",
      "Epoch 90/600, Loss: 1.0055317878723145\n",
      "Epoch 91/600, Loss: 1.0022882223129272\n",
      "Epoch 92/600, Loss: 1.0000996589660645\n",
      "Epoch 93/600, Loss: 0.9995397925376892\n",
      "Epoch 94/600, Loss: 1.0026520490646362\n",
      "Epoch 95/600, Loss: 0.9980632662773132\n",
      "Epoch 96/600, Loss: 1.0032354593276978\n",
      "Epoch 97/600, Loss: 0.9994181990623474\n",
      "Epoch 98/600, Loss: 0.9997402429580688\n",
      "Epoch 99/600, Loss: 0.9984583854675293\n",
      "Epoch 100/600, Loss: 0.9996122121810913\n",
      "Epoch 101/600, Loss: 1.002540111541748\n",
      "Epoch 102/600, Loss: 1.0054396390914917\n",
      "Epoch 103/600, Loss: 1.0006585121154785\n",
      "Epoch 104/600, Loss: 0.994825541973114\n",
      "Epoch 105/600, Loss: 1.0011647939682007\n",
      "Epoch 106/600, Loss: 0.994837760925293\n",
      "Epoch 107/600, Loss: 0.9961576461791992\n",
      "Epoch 108/600, Loss: 0.9991317391395569\n",
      "Epoch 109/600, Loss: 0.9971078634262085\n",
      "Epoch 110/600, Loss: 1.000315546989441\n",
      "Epoch 111/600, Loss: 1.0056616067886353\n",
      "Epoch 112/600, Loss: 0.9986318349838257\n",
      "Epoch 113/600, Loss: 1.0029513835906982\n",
      "Epoch 114/600, Loss: 1.000210165977478\n",
      "Epoch 115/600, Loss: 1.0039284229278564\n",
      "Epoch 116/600, Loss: 0.9956163167953491\n",
      "Epoch 117/600, Loss: 1.0010746717453003\n",
      "Epoch 118/600, Loss: 0.9999406337738037\n",
      "Epoch 119/600, Loss: 1.0009794235229492\n",
      "Epoch 120/600, Loss: 1.0054603815078735\n",
      "Epoch 121/600, Loss: 0.9953165650367737\n",
      "Epoch 122/600, Loss: 1.0033364295959473\n",
      "Epoch 123/600, Loss: 0.9999514818191528\n",
      "Epoch 124/600, Loss: 1.0015385150909424\n",
      "Epoch 125/600, Loss: 0.9999669194221497\n",
      "Epoch 126/600, Loss: 1.0025537014007568\n",
      "Epoch 127/600, Loss: 0.9994253516197205\n",
      "Epoch 128/600, Loss: 0.9977507591247559\n",
      "Epoch 129/600, Loss: 0.9970645904541016\n",
      "Epoch 130/600, Loss: 1.0019713640213013\n",
      "Epoch 131/600, Loss: 0.9982454776763916\n",
      "Epoch 132/600, Loss: 1.0022250413894653\n",
      "Epoch 133/600, Loss: 0.9999576807022095\n",
      "Epoch 134/600, Loss: 1.0048999786376953\n",
      "Epoch 135/600, Loss: 1.0017626285552979\n",
      "Epoch 136/600, Loss: 1.0050714015960693\n",
      "Epoch 137/600, Loss: 1.000695824623108\n",
      "Epoch 138/600, Loss: 1.0002145767211914\n",
      "Epoch 139/600, Loss: 0.9963501691818237\n",
      "Epoch 140/600, Loss: 0.9993036389350891\n",
      "Epoch 141/600, Loss: 1.0031019449234009\n",
      "Epoch 142/600, Loss: 1.0031218528747559\n",
      "Epoch 143/600, Loss: 1.004264235496521\n",
      "Epoch 144/600, Loss: 0.9943931102752686\n",
      "Epoch 145/600, Loss: 0.9943048357963562\n",
      "Epoch 146/600, Loss: 0.9990730881690979\n",
      "Epoch 147/600, Loss: 1.0002626180648804\n",
      "Epoch 148/600, Loss: 1.001583218574524\n",
      "Epoch 149/600, Loss: 0.9965778589248657\n",
      "Epoch 150/600, Loss: 0.997941792011261\n",
      "Epoch 151/600, Loss: 1.001814842224121\n",
      "Epoch 152/600, Loss: 1.0015519857406616\n",
      "Epoch 153/600, Loss: 1.0014318227767944\n",
      "Epoch 154/600, Loss: 1.0027459859848022\n",
      "Epoch 155/600, Loss: 1.0014580488204956\n",
      "Epoch 156/600, Loss: 0.9973196983337402\n",
      "Epoch 157/600, Loss: 0.9984411001205444\n",
      "Epoch 158/600, Loss: 0.99854975938797\n",
      "Epoch 159/600, Loss: 1.0014169216156006\n",
      "Epoch 160/600, Loss: 0.9977980256080627\n",
      "Epoch 161/600, Loss: 1.0016858577728271\n",
      "Epoch 162/600, Loss: 0.9999816417694092\n",
      "Epoch 163/600, Loss: 1.0012290477752686\n",
      "Epoch 164/600, Loss: 1.0028325319290161\n",
      "Epoch 165/600, Loss: 0.9999139308929443\n",
      "Epoch 166/600, Loss: 0.9935017228126526\n",
      "Epoch 167/600, Loss: 1.0022000074386597\n",
      "Epoch 168/600, Loss: 1.0062377452850342\n",
      "Epoch 169/600, Loss: 1.003659725189209\n",
      "Epoch 170/600, Loss: 0.997346043586731\n",
      "Epoch 171/600, Loss: 1.0021077394485474\n",
      "Epoch 172/600, Loss: 1.0011276006698608\n",
      "Epoch 173/600, Loss: 1.0017813444137573\n",
      "Epoch 174/600, Loss: 0.9992814064025879\n",
      "Epoch 175/600, Loss: 1.000762939453125\n",
      "Epoch 176/600, Loss: 1.002788782119751\n",
      "Epoch 177/600, Loss: 1.0016909837722778\n",
      "Epoch 178/600, Loss: 1.0018610954284668\n",
      "Epoch 179/600, Loss: 0.9985859394073486\n",
      "Epoch 180/600, Loss: 1.0026087760925293\n",
      "Epoch 181/600, Loss: 0.9987865686416626\n",
      "Epoch 182/600, Loss: 1.0040425062179565\n",
      "Epoch 183/600, Loss: 1.0004005432128906\n",
      "Epoch 184/600, Loss: 0.9954922795295715\n",
      "Epoch 185/600, Loss: 0.999779462814331\n",
      "Epoch 186/600, Loss: 0.9982327222824097\n",
      "Epoch 187/600, Loss: 1.0009552240371704\n",
      "Epoch 188/600, Loss: 1.0001425743103027\n",
      "Epoch 189/600, Loss: 1.0044399499893188\n",
      "Epoch 190/600, Loss: 1.0006940364837646\n",
      "Epoch 191/600, Loss: 1.0021523237228394\n",
      "Epoch 192/600, Loss: 1.0028637647628784\n",
      "Epoch 193/600, Loss: 1.0016533136367798\n",
      "Epoch 194/600, Loss: 0.9962608218193054\n",
      "Epoch 195/600, Loss: 1.003830075263977\n",
      "Epoch 196/600, Loss: 1.0022729635238647\n",
      "Epoch 197/600, Loss: 0.9991346001625061\n",
      "Epoch 198/600, Loss: 1.0060051679611206\n",
      "Epoch 199/600, Loss: 1.0002635717391968\n",
      "Epoch 200/600, Loss: 0.9999414682388306\n",
      "Epoch 201/600, Loss: 1.0016015768051147\n",
      "Epoch 202/600, Loss: 1.0032966136932373\n",
      "Epoch 203/600, Loss: 1.0033371448516846\n",
      "Epoch 204/600, Loss: 0.9963672161102295\n",
      "Epoch 205/600, Loss: 0.9976614117622375\n",
      "Epoch 206/600, Loss: 0.9964749813079834\n",
      "Epoch 207/600, Loss: 0.9985150098800659\n",
      "Epoch 208/600, Loss: 0.9993678331375122\n",
      "Epoch 209/600, Loss: 1.0020368099212646\n",
      "Epoch 210/600, Loss: 0.9995342493057251\n",
      "Epoch 211/600, Loss: 0.9995400905609131\n",
      "Epoch 212/600, Loss: 1.0002962350845337\n",
      "Epoch 213/600, Loss: 0.9966654777526855\n",
      "Epoch 214/600, Loss: 1.0035064220428467\n",
      "Epoch 215/600, Loss: 1.0005073547363281\n",
      "Epoch 216/600, Loss: 0.9994714260101318\n",
      "Epoch 217/600, Loss: 0.9979895353317261\n",
      "Epoch 218/600, Loss: 0.9999534487724304\n",
      "Epoch 219/600, Loss: 1.0000507831573486\n",
      "Epoch 220/600, Loss: 1.0063939094543457\n",
      "Epoch 221/600, Loss: 1.0008848905563354\n",
      "Epoch 222/600, Loss: 0.9969690442085266\n",
      "Epoch 223/600, Loss: 0.9959125518798828\n",
      "Epoch 224/600, Loss: 1.0039849281311035\n",
      "Epoch 225/600, Loss: 0.9994462132453918\n",
      "Epoch 226/600, Loss: 1.0022321939468384\n",
      "Epoch 227/600, Loss: 0.9994643926620483\n",
      "Epoch 228/600, Loss: 0.9970927238464355\n",
      "Epoch 229/600, Loss: 1.001308798789978\n",
      "Epoch 230/600, Loss: 1.0023102760314941\n",
      "Epoch 231/600, Loss: 0.9962306022644043\n",
      "Epoch 232/600, Loss: 0.9950515031814575\n",
      "Epoch 233/600, Loss: 0.9970508217811584\n",
      "Epoch 234/600, Loss: 1.0000015497207642\n",
      "Epoch 235/600, Loss: 0.9975703358650208\n",
      "Epoch 236/600, Loss: 1.0003533363342285\n",
      "Epoch 237/600, Loss: 0.9977872967720032\n",
      "Epoch 238/600, Loss: 0.9993675947189331\n",
      "Epoch 239/600, Loss: 0.9931140542030334\n",
      "Epoch 240/600, Loss: 1.002418041229248\n",
      "Epoch 241/600, Loss: 1.003071904182434\n",
      "Epoch 242/600, Loss: 0.9975235462188721\n",
      "Epoch 243/600, Loss: 1.0054457187652588\n",
      "Epoch 244/600, Loss: 0.9990839958190918\n",
      "Epoch 245/600, Loss: 0.9999176263809204\n",
      "Epoch 246/600, Loss: 0.9988328218460083\n",
      "Epoch 247/600, Loss: 0.9996758699417114\n",
      "Epoch 248/600, Loss: 0.9950792193412781\n",
      "Epoch 249/600, Loss: 1.0026874542236328\n",
      "Epoch 250/600, Loss: 1.0004456043243408\n",
      "Epoch 251/600, Loss: 0.9950200915336609\n",
      "Epoch 252/600, Loss: 0.9949293732643127\n",
      "Epoch 253/600, Loss: 0.9968095421791077\n",
      "Epoch 254/600, Loss: 0.9971004724502563\n",
      "Epoch 255/600, Loss: 0.9973841905593872\n",
      "Epoch 256/600, Loss: 0.9989447593688965\n",
      "Epoch 257/600, Loss: 0.9996828436851501\n",
      "Epoch 258/600, Loss: 0.9997960925102234\n",
      "Epoch 259/600, Loss: 1.0012983083724976\n",
      "Epoch 260/600, Loss: 0.9996948838233948\n",
      "Epoch 261/600, Loss: 0.9955368638038635\n",
      "Epoch 262/600, Loss: 1.0007482767105103\n",
      "Epoch 263/600, Loss: 0.9975173473358154\n",
      "Epoch 264/600, Loss: 0.9973142743110657\n",
      "Epoch 265/600, Loss: 0.9954145550727844\n",
      "Epoch 266/600, Loss: 0.9971047043800354\n",
      "Epoch 267/600, Loss: 1.0029819011688232\n",
      "Epoch 268/600, Loss: 0.997851550579071\n",
      "Epoch 269/600, Loss: 1.0014278888702393\n",
      "Epoch 270/600, Loss: 0.9964601397514343\n",
      "Epoch 271/600, Loss: 0.9981429576873779\n",
      "Epoch 272/600, Loss: 1.0046886205673218\n",
      "Epoch 273/600, Loss: 0.9996844530105591\n",
      "Epoch 274/600, Loss: 0.9966071844100952\n",
      "Epoch 275/600, Loss: 1.0003362894058228\n",
      "Epoch 276/600, Loss: 0.9997146129608154\n",
      "Epoch 277/600, Loss: 1.0030686855316162\n",
      "Epoch 278/600, Loss: 1.0009397268295288\n",
      "Epoch 279/600, Loss: 0.9984639286994934\n",
      "Epoch 280/600, Loss: 1.0027611255645752\n",
      "Epoch 281/600, Loss: 0.9982703924179077\n",
      "Epoch 282/600, Loss: 1.002631664276123\n",
      "Epoch 283/600, Loss: 0.9990936517715454\n",
      "Epoch 284/600, Loss: 1.00281822681427\n",
      "Epoch 285/600, Loss: 1.0011900663375854\n",
      "Epoch 286/600, Loss: 0.9981874227523804\n",
      "Epoch 287/600, Loss: 1.0014456510543823\n",
      "Epoch 288/600, Loss: 0.9991143941879272\n",
      "Epoch 289/600, Loss: 1.0028152465820312\n",
      "Epoch 290/600, Loss: 0.9925951957702637\n",
      "Epoch 291/600, Loss: 1.0021556615829468\n",
      "Epoch 292/600, Loss: 1.0000449419021606\n",
      "Epoch 293/600, Loss: 0.9970455765724182\n",
      "Epoch 294/600, Loss: 0.9951009750366211\n",
      "Epoch 295/600, Loss: 0.9976524710655212\n",
      "Epoch 296/600, Loss: 0.9997566342353821\n",
      "Epoch 297/600, Loss: 1.0023092031478882\n",
      "Epoch 298/600, Loss: 0.9932446479797363\n",
      "Epoch 299/600, Loss: 0.9975522756576538\n",
      "Epoch 300/600, Loss: 1.003286361694336\n",
      "Epoch 301/600, Loss: 0.9984769821166992\n",
      "Epoch 302/600, Loss: 0.9996836185455322\n",
      "Epoch 303/600, Loss: 0.999082624912262\n",
      "Epoch 304/600, Loss: 1.003007173538208\n",
      "Epoch 305/600, Loss: 1.0022058486938477\n",
      "Epoch 306/600, Loss: 0.9943744540214539\n",
      "Epoch 307/600, Loss: 1.0000227689743042\n",
      "Epoch 308/600, Loss: 0.9966132640838623\n",
      "Epoch 309/600, Loss: 0.998876690864563\n",
      "Epoch 310/600, Loss: 0.9971887469291687\n",
      "Epoch 311/600, Loss: 0.9981156587600708\n",
      "Epoch 312/600, Loss: 0.9961607456207275\n",
      "Epoch 313/600, Loss: 1.0016202926635742\n",
      "Epoch 314/600, Loss: 1.0014090538024902\n",
      "Epoch 315/600, Loss: 0.9983758926391602\n",
      "Epoch 316/600, Loss: 1.0005431175231934\n",
      "Epoch 317/600, Loss: 0.9987264275550842\n",
      "Epoch 318/600, Loss: 0.998016357421875\n",
      "Epoch 319/600, Loss: 1.0033410787582397\n",
      "Epoch 320/600, Loss: 0.9964964985847473\n",
      "Epoch 321/600, Loss: 0.9954290390014648\n",
      "Epoch 322/600, Loss: 0.9972565174102783\n",
      "Epoch 323/600, Loss: 0.9995030164718628\n",
      "Epoch 324/600, Loss: 1.0015474557876587\n",
      "Epoch 325/600, Loss: 1.0037424564361572\n",
      "Epoch 326/600, Loss: 1.000678300857544\n",
      "Epoch 327/600, Loss: 0.9978352189064026\n",
      "Epoch 328/600, Loss: 0.9992347359657288\n",
      "Epoch 329/600, Loss: 0.9998749494552612\n",
      "Epoch 330/600, Loss: 0.9991347789764404\n",
      "Epoch 331/600, Loss: 0.9994208812713623\n",
      "Epoch 332/600, Loss: 0.9988354444503784\n",
      "Epoch 333/600, Loss: 0.9970763921737671\n",
      "Epoch 334/600, Loss: 0.998896598815918\n",
      "Epoch 335/600, Loss: 1.0005019903182983\n",
      "Epoch 336/600, Loss: 1.0051673650741577\n",
      "Epoch 337/600, Loss: 1.0030162334442139\n",
      "Epoch 338/600, Loss: 0.9980823993682861\n",
      "Epoch 339/600, Loss: 0.9984469413757324\n",
      "Epoch 340/600, Loss: 1.0034294128417969\n",
      "Epoch 341/600, Loss: 0.9954454302787781\n",
      "Epoch 342/600, Loss: 1.0026352405548096\n",
      "Epoch 343/600, Loss: 0.9971546530723572\n",
      "Epoch 344/600, Loss: 1.0019022226333618\n",
      "Epoch 345/600, Loss: 0.9985318779945374\n",
      "Epoch 346/600, Loss: 0.997541069984436\n",
      "Epoch 347/600, Loss: 1.0045760869979858\n",
      "Epoch 348/600, Loss: 1.002584457397461\n",
      "Epoch 349/600, Loss: 0.994720995426178\n",
      "Epoch 350/600, Loss: 0.9993661642074585\n",
      "Epoch 351/600, Loss: 0.9932801723480225\n",
      "Epoch 352/600, Loss: 1.0049495697021484\n",
      "Epoch 353/600, Loss: 1.003952980041504\n",
      "Epoch 354/600, Loss: 1.0012198686599731\n",
      "Epoch 355/600, Loss: 1.0026813745498657\n",
      "Epoch 356/600, Loss: 0.997916042804718\n",
      "Epoch 357/600, Loss: 0.9942231774330139\n",
      "Epoch 358/600, Loss: 0.9994705319404602\n",
      "Epoch 359/600, Loss: 0.9997596144676208\n",
      "Epoch 360/600, Loss: 0.9971683025360107\n",
      "Epoch 361/600, Loss: 0.9998636245727539\n",
      "Epoch 362/600, Loss: 0.9994754195213318\n",
      "Epoch 363/600, Loss: 1.0023735761642456\n",
      "Epoch 364/600, Loss: 1.0025832653045654\n",
      "Epoch 365/600, Loss: 1.003800392150879\n",
      "Epoch 366/600, Loss: 1.0001963376998901\n",
      "Epoch 367/600, Loss: 1.0046379566192627\n",
      "Epoch 368/600, Loss: 0.9955040216445923\n",
      "Epoch 369/600, Loss: 0.9998391270637512\n",
      "Epoch 370/600, Loss: 1.0022777318954468\n",
      "Epoch 371/600, Loss: 0.9967470169067383\n",
      "Epoch 372/600, Loss: 0.9959290027618408\n",
      "Epoch 373/600, Loss: 0.9940800070762634\n",
      "Epoch 374/600, Loss: 1.0021477937698364\n",
      "Epoch 375/600, Loss: 0.9949706196784973\n",
      "Epoch 376/600, Loss: 0.9955597519874573\n",
      "Epoch 377/600, Loss: 0.9990110397338867\n",
      "Epoch 378/600, Loss: 1.0012661218643188\n",
      "Epoch 379/600, Loss: 0.9989990592002869\n",
      "Epoch 380/600, Loss: 0.9969618320465088\n",
      "Epoch 381/600, Loss: 1.0009914636611938\n",
      "Epoch 382/600, Loss: 1.0032674074172974\n",
      "Epoch 383/600, Loss: 0.9968793392181396\n",
      "Epoch 384/600, Loss: 1.0010027885437012\n",
      "Epoch 385/600, Loss: 1.0007634162902832\n",
      "Epoch 386/600, Loss: 1.0034234523773193\n",
      "Epoch 387/600, Loss: 1.0058660507202148\n",
      "Epoch 388/600, Loss: 0.9956086874008179\n",
      "Epoch 389/600, Loss: 1.0012333393096924\n",
      "Epoch 390/600, Loss: 1.0017180442810059\n",
      "Epoch 391/600, Loss: 0.9948920607566833\n",
      "Epoch 392/600, Loss: 1.0012320280075073\n",
      "Epoch 393/600, Loss: 1.0032602548599243\n",
      "Epoch 394/600, Loss: 0.9993154406547546\n",
      "Epoch 395/600, Loss: 1.0023740530014038\n",
      "Epoch 396/600, Loss: 0.9986717700958252\n",
      "Epoch 397/600, Loss: 0.9970869421958923\n",
      "Epoch 398/600, Loss: 1.0021799802780151\n",
      "Epoch 399/600, Loss: 1.0005220174789429\n",
      "Epoch 400/600, Loss: 1.0038844347000122\n",
      "Epoch 401/600, Loss: 0.9935854077339172\n",
      "Epoch 402/600, Loss: 0.9998916983604431\n",
      "Epoch 403/600, Loss: 0.9959897994995117\n",
      "Epoch 404/600, Loss: 1.0027790069580078\n",
      "Epoch 405/600, Loss: 1.0023525953292847\n",
      "Epoch 406/600, Loss: 1.0051918029785156\n",
      "Epoch 407/600, Loss: 1.0029181241989136\n",
      "Epoch 408/600, Loss: 1.0004022121429443\n",
      "Epoch 409/600, Loss: 1.0055956840515137\n",
      "Epoch 410/600, Loss: 1.0048388242721558\n",
      "Epoch 411/600, Loss: 0.9969537854194641\n",
      "Epoch 412/600, Loss: 1.006495714187622\n",
      "Epoch 413/600, Loss: 1.0016875267028809\n",
      "Epoch 414/600, Loss: 0.9983585476875305\n",
      "Epoch 415/600, Loss: 1.0014784336090088\n",
      "Epoch 416/600, Loss: 1.0014934539794922\n",
      "Epoch 417/600, Loss: 1.0040324926376343\n",
      "Epoch 418/600, Loss: 0.9968680739402771\n",
      "Epoch 419/600, Loss: 1.0039427280426025\n",
      "Epoch 420/600, Loss: 0.9971886277198792\n",
      "Epoch 421/600, Loss: 1.0015239715576172\n",
      "Epoch 422/600, Loss: 0.9977346062660217\n",
      "Epoch 423/600, Loss: 1.0005906820297241\n",
      "Epoch 424/600, Loss: 0.999168336391449\n",
      "Epoch 425/600, Loss: 0.9975335597991943\n",
      "Epoch 426/600, Loss: 1.0005499124526978\n",
      "Epoch 427/600, Loss: 1.0000618696212769\n",
      "Epoch 428/600, Loss: 0.9960035085678101\n",
      "Epoch 429/600, Loss: 0.9980273246765137\n",
      "Epoch 430/600, Loss: 0.9998683929443359\n",
      "Epoch 431/600, Loss: 0.9958139061927795\n",
      "Epoch 432/600, Loss: 0.9950599670410156\n",
      "Epoch 433/600, Loss: 1.0034507513046265\n",
      "Epoch 434/600, Loss: 0.9997556209564209\n",
      "Epoch 435/600, Loss: 0.9993641376495361\n",
      "Epoch 436/600, Loss: 0.993256688117981\n",
      "Epoch 437/600, Loss: 0.9989832043647766\n",
      "Epoch 438/600, Loss: 1.0045855045318604\n",
      "Epoch 439/600, Loss: 0.9958386421203613\n",
      "Epoch 440/600, Loss: 1.0012210607528687\n",
      "Epoch 441/600, Loss: 1.0035556554794312\n",
      "Epoch 442/600, Loss: 0.9955611824989319\n",
      "Epoch 443/600, Loss: 0.9972304701805115\n",
      "Epoch 444/600, Loss: 1.0006685256958008\n",
      "Epoch 445/600, Loss: 0.9993119239807129\n",
      "Epoch 446/600, Loss: 1.0031160116195679\n",
      "Epoch 447/600, Loss: 0.9980146288871765\n",
      "Epoch 448/600, Loss: 0.9980199933052063\n",
      "Epoch 449/600, Loss: 1.0008339881896973\n",
      "Epoch 450/600, Loss: 0.9978723526000977\n",
      "Epoch 451/600, Loss: 1.0009536743164062\n",
      "Epoch 452/600, Loss: 1.0006004571914673\n",
      "Epoch 453/600, Loss: 1.0018280744552612\n",
      "Epoch 454/600, Loss: 1.003380298614502\n",
      "Epoch 455/600, Loss: 1.0031379461288452\n",
      "Epoch 456/600, Loss: 0.9982255101203918\n",
      "Epoch 457/600, Loss: 0.9977411031723022\n",
      "Epoch 458/600, Loss: 1.0006000995635986\n",
      "Epoch 459/600, Loss: 0.9997040033340454\n",
      "Epoch 460/600, Loss: 1.0065678358078003\n",
      "Epoch 461/600, Loss: 1.0068016052246094\n",
      "Epoch 462/600, Loss: 0.9968157410621643\n",
      "Epoch 463/600, Loss: 1.0013748407363892\n",
      "Epoch 464/600, Loss: 0.9976466298103333\n",
      "Epoch 465/600, Loss: 0.9969568848609924\n",
      "Epoch 466/600, Loss: 0.9984734058380127\n",
      "Epoch 467/600, Loss: 0.9981904625892639\n",
      "Epoch 468/600, Loss: 1.000800609588623\n",
      "Epoch 469/600, Loss: 1.0057554244995117\n",
      "Epoch 470/600, Loss: 1.0015977621078491\n",
      "Epoch 471/600, Loss: 0.9993294477462769\n",
      "Epoch 472/600, Loss: 1.0022019147872925\n",
      "Epoch 473/600, Loss: 0.9984577894210815\n",
      "Epoch 474/600, Loss: 0.9964509606361389\n",
      "Epoch 475/600, Loss: 1.000486135482788\n",
      "Epoch 476/600, Loss: 0.9996399283409119\n",
      "Epoch 477/600, Loss: 0.9972450733184814\n",
      "Epoch 478/600, Loss: 1.0022892951965332\n",
      "Epoch 479/600, Loss: 0.9996520280838013\n",
      "Epoch 480/600, Loss: 1.0037840604782104\n",
      "Epoch 481/600, Loss: 1.0036933422088623\n",
      "Epoch 482/600, Loss: 1.0015172958374023\n",
      "Epoch 483/600, Loss: 0.9960222840309143\n",
      "Epoch 484/600, Loss: 0.9995922446250916\n",
      "Epoch 485/600, Loss: 1.0021493434906006\n",
      "Epoch 486/600, Loss: 1.0010873079299927\n",
      "Epoch 487/600, Loss: 1.0011630058288574\n",
      "Epoch 488/600, Loss: 1.0020860433578491\n",
      "Epoch 489/600, Loss: 1.0013549327850342\n",
      "Epoch 490/600, Loss: 0.996581494808197\n",
      "Epoch 491/600, Loss: 0.9988301992416382\n",
      "Epoch 492/600, Loss: 1.004137635231018\n",
      "Epoch 493/600, Loss: 0.9968510866165161\n",
      "Epoch 494/600, Loss: 1.0005767345428467\n",
      "Epoch 495/600, Loss: 1.0008454322814941\n",
      "Epoch 496/600, Loss: 0.9985254406929016\n",
      "Epoch 497/600, Loss: 1.0050729513168335\n",
      "Epoch 498/600, Loss: 0.9997730255126953\n",
      "Epoch 499/600, Loss: 1.0045757293701172\n",
      "Epoch 500/600, Loss: 0.9936100244522095\n",
      "Epoch 501/600, Loss: 0.9983652830123901\n",
      "Epoch 502/600, Loss: 1.0016045570373535\n",
      "Epoch 503/600, Loss: 1.0001493692398071\n",
      "Epoch 504/600, Loss: 0.9999183416366577\n",
      "Epoch 505/600, Loss: 1.0042846202850342\n",
      "Epoch 506/600, Loss: 1.0009195804595947\n",
      "Epoch 507/600, Loss: 1.0016961097717285\n",
      "Epoch 508/600, Loss: 0.997627854347229\n",
      "Epoch 509/600, Loss: 1.000405192375183\n",
      "Epoch 510/600, Loss: 1.0001771450042725\n",
      "Epoch 511/600, Loss: 0.9956979751586914\n",
      "Epoch 512/600, Loss: 1.0023455619812012\n",
      "Epoch 513/600, Loss: 1.0006531476974487\n",
      "Epoch 514/600, Loss: 1.001715064048767\n",
      "Epoch 515/600, Loss: 1.0000334978103638\n",
      "Epoch 516/600, Loss: 1.001882791519165\n",
      "Epoch 517/600, Loss: 1.0001059770584106\n",
      "Epoch 518/600, Loss: 0.9981144666671753\n",
      "Epoch 519/600, Loss: 1.0013471841812134\n",
      "Epoch 520/600, Loss: 1.0009669065475464\n",
      "Epoch 521/600, Loss: 0.9998044967651367\n",
      "Epoch 522/600, Loss: 1.0011919736862183\n",
      "Epoch 523/600, Loss: 0.997248649597168\n",
      "Epoch 524/600, Loss: 0.998237133026123\n",
      "Epoch 525/600, Loss: 0.9975185990333557\n",
      "Epoch 526/600, Loss: 0.9992282390594482\n",
      "Epoch 527/600, Loss: 1.0008646249771118\n",
      "Epoch 528/600, Loss: 1.00026535987854\n",
      "Epoch 529/600, Loss: 1.004760980606079\n",
      "Epoch 530/600, Loss: 0.9975044131278992\n",
      "Epoch 531/600, Loss: 0.9962475299835205\n",
      "Epoch 532/600, Loss: 0.9983466267585754\n",
      "Epoch 533/600, Loss: 0.997270941734314\n",
      "Epoch 534/600, Loss: 0.9976258277893066\n",
      "Epoch 535/600, Loss: 0.9975085854530334\n",
      "Epoch 536/600, Loss: 1.0035425424575806\n",
      "Epoch 537/600, Loss: 1.000450611114502\n",
      "Epoch 538/600, Loss: 1.0013624429702759\n",
      "Epoch 539/600, Loss: 0.9992060661315918\n",
      "Epoch 540/600, Loss: 1.005333423614502\n",
      "Epoch 541/600, Loss: 1.0018854141235352\n",
      "Epoch 542/600, Loss: 1.0064687728881836\n",
      "Epoch 543/600, Loss: 0.9992555379867554\n",
      "Epoch 544/600, Loss: 0.9957649111747742\n",
      "Epoch 545/600, Loss: 1.0051531791687012\n",
      "Epoch 546/600, Loss: 0.999943733215332\n",
      "Epoch 547/600, Loss: 1.0009483098983765\n",
      "Epoch 548/600, Loss: 1.001825213432312\n",
      "Epoch 549/600, Loss: 1.0039684772491455\n",
      "Epoch 550/600, Loss: 0.9965602159500122\n",
      "Epoch 551/600, Loss: 1.0008635520935059\n",
      "Epoch 552/600, Loss: 1.0006675720214844\n",
      "Epoch 553/600, Loss: 0.9997804164886475\n",
      "Epoch 554/600, Loss: 0.9983033537864685\n",
      "Epoch 555/600, Loss: 1.00232994556427\n",
      "Epoch 556/600, Loss: 1.0013777017593384\n",
      "Epoch 557/600, Loss: 0.9954609870910645\n",
      "Epoch 558/600, Loss: 0.9987859725952148\n",
      "Epoch 559/600, Loss: 0.9984089136123657\n",
      "Epoch 560/600, Loss: 0.9981036186218262\n",
      "Epoch 561/600, Loss: 0.9990475177764893\n",
      "Epoch 562/600, Loss: 0.9988159537315369\n",
      "Epoch 563/600, Loss: 0.9981774091720581\n",
      "Epoch 564/600, Loss: 1.003906011581421\n",
      "Epoch 565/600, Loss: 1.0028308629989624\n",
      "Epoch 566/600, Loss: 0.9946098923683167\n",
      "Epoch 567/600, Loss: 1.000923752784729\n",
      "Epoch 568/600, Loss: 1.0006334781646729\n",
      "Epoch 569/600, Loss: 1.0018808841705322\n",
      "Epoch 570/600, Loss: 0.9983688592910767\n",
      "Epoch 571/600, Loss: 0.9985466003417969\n",
      "Epoch 572/600, Loss: 1.0010818243026733\n",
      "Epoch 573/600, Loss: 1.005326509475708\n",
      "Epoch 574/600, Loss: 0.9967208504676819\n",
      "Epoch 575/600, Loss: 0.9993748068809509\n",
      "Epoch 576/600, Loss: 1.0010185241699219\n",
      "Epoch 577/600, Loss: 0.9964337348937988\n",
      "Epoch 578/600, Loss: 1.0072696208953857\n",
      "Epoch 579/600, Loss: 0.9958688616752625\n",
      "Epoch 580/600, Loss: 0.9994052648544312\n",
      "Epoch 581/600, Loss: 0.998874306678772\n",
      "Epoch 582/600, Loss: 1.0007659196853638\n",
      "Epoch 583/600, Loss: 1.005906343460083\n",
      "Epoch 584/600, Loss: 0.9986732602119446\n",
      "Epoch 585/600, Loss: 0.9986493587493896\n",
      "Epoch 586/600, Loss: 0.998979389667511\n",
      "Epoch 587/600, Loss: 0.994231104850769\n",
      "Epoch 588/600, Loss: 1.0008045434951782\n",
      "Epoch 589/600, Loss: 0.9993309378623962\n",
      "Epoch 590/600, Loss: 0.9990697503089905\n",
      "Epoch 591/600, Loss: 0.996202290058136\n",
      "Epoch 592/600, Loss: 0.9992393255233765\n",
      "Epoch 593/600, Loss: 1.0068097114562988\n",
      "Epoch 594/600, Loss: 0.9982472062110901\n",
      "Epoch 595/600, Loss: 1.0026651620864868\n",
      "Epoch 596/600, Loss: 0.9940475225448608\n",
      "Epoch 597/600, Loss: 1.0031076669692993\n",
      "Epoch 598/600, Loss: 1.0013130903244019\n",
      "Epoch 599/600, Loss: 1.0017441511154175\n",
      "Epoch 600/600, Loss: 0.9989289045333862\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "from nets.attentionLDM import CosineScheduler, UNet3D, LatentDiffusionModel\n",
    "from latent_dataset import LatentEmbeddingsDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "cuda_id = \"cuda:\" + str(6)\n",
    "device = torch.device(cuda_id)\n",
    "_date = datetime.now().strftime(\"%d-%H-%M\")\n",
    "#log_dir = f\"./runs/diff_latent_{_date}\"\n",
    "log_dir = \"./runs/test\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Define paths to your latent embedding directories\n",
    "latents_dir = '/mnt/disk1/hjlee/orhun/data/BRATS_Preprocessed/CorrespondingLatents/All'  # Replace with the correct path\n",
    "latents_single_dir = '/mnt/disk1/hjlee/orhun/data/BRATS_Preprocessed/CorrespondingLatents/T1C'  # Replace with the correct path\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "num_epochs = 600\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = LatentEmbeddingsDataset(latents_dir, latents_single_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "# Initialize components\n",
    "cosine_scheduler = CosineScheduler(timesteps=1000, device= device )\n",
    "unet = UNet3D(in_channels=768, base_channels=128).to(device)\n",
    "ldm = LatentDiffusionModel(unet, cosine_scheduler).to(device)\n",
    "optimizer = torch.optim.Adam(ldm.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        t = torch.randint(0, cosine_scheduler.timesteps, (src.size(0),), device=src.device)\n",
    "\n",
    "        # Noise injection\n",
    "        noise = torch.randn_like(src).to(device)\n",
    "        alpha_bar_t = cosine_scheduler.get_alpha_bar(t).view(-1, 1, 1, 1, 1)\n",
    "        noisy_src = torch.sqrt(alpha_bar_t) * src + torch.sqrt(1 - alpha_bar_t) * noise\n",
    "\n",
    "        # Predict noise\n",
    "        noise_pred = ldm(noisy_src, t)\n",
    "\n",
    "        # MSE Loss\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
